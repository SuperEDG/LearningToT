# 思维树：深思熟虑的问题解决方法

## 摘要

语言模型正在越来越多地被用于广泛的任务中进行一般性问题解决，但在推理过程中，它们仍然局限于从左到右的基于令牌级别的决策过程。这意味着它们在需要探索、战略性预见或初始决策起关键作用的任务中可能表现不佳。为了克服这些挑战，我们引入了一种新的语言模型推理框架，“思维树”（Tree of Thoughts，简称ToT），它在流行的“思维链”方法之上进行了泛化，用于提示语言模型，并允许对作为解决问题中间步骤的连贯文本单元（“思维”）进行探索。ToT允许语言模型通过考虑多个不同的推理路径并自我评估选择来进行深思熟虑的决策，从而决定下一步行动，同时在必要时进行前瞻或回溯，以做出全局性的选择。

我们的实验表明，ToT显著提高了语言模型在需要非琐碎规划或搜索的三个新任务上的问题解决能力：24点游戏、创意写作和迷你纵横字谜。例如，在24点游戏中，使用思维链提示的GPT-4只解决了4%的任务，而我们的方法达到了74%的成功率。所有提示的代码仓库如下：[https://github.com/ysymyth/tree-of-thought-llm](https://github.com/ysymyth/tree-of-thought-llm)。

## 1 引言

最初被设计用来生成文本的语言模型（LMs），如GPT [22; 23; 1; 20] 和 PaLM [5] 的扩展版本，已被证明越来越有能力执行越来越广泛的任务，这些任务需要数学的、符号的、常识性的和知识性的推理。或许令人惊讶的是，所有这些进展的基础仍然是生成文本的原始自回归机制，该机制一次做出一个，从左到右的令牌级决策。这样一个简单的机制是否足以构建一个通用的问题解决者？如果不是，什么问题会挑战当前的范式，应该有什么替代机制？

关于人类认知的文献为这些问题提供了一些线索。关于“双重过程”的研究表明，人们在处理决策时有两种模式——一种快速的、自动的、无意识的模式（“系统1”）和一种缓慢的、深思熟虑的、有意识的模式（“系统2”）[27; 28; 13; 12]。这两种模式之前已经与机器学习中使用的各种数学模型相联系。例如，关于人类和其他动物的强化学习的研究已经探讨了它们从事联想的“无模型”学习或更多深思熟虑的“基于模型”的规划的情况[6]。LMs的简单联想令牌级选择也让人想起“系统1”，因此可能会受益于更深思熟虑的“系统2”规划过程的增强，该过程（1）保持并探索当前选择的多种替代方案，而不仅仅是选择一个；以及（2）评估其当前状态，并积极预见或回溯，以做出更全球性的决策。

为了设计这样一个规划过程，我们回到人工智能（和认知科学）的起源，从1950年代Newell、Shaw和Simon开始探索的规划过程中汲取灵感[18; 19]。Newell和他的同事们将问题解决[18]描述为通过一个组合问题空间的搜索，该空间被表示为一棵树。因此，我们提出了一种用于语言模型的通用问题解决框架——“思维树”（Tree of Thoughts，简称ToT）。如图1所示，尽管现有方法（下面详细说明）采样连续的语言序列进行问题解决，但ToT积极维护一个思维树，其中每个_思维_都是一个连贯的语言序列，作为解决问题的中间步骤（表1）。这样一个高级语义单元允许LM通过一个也用语言实例化的深思熟虑的推理过程，自我评估不同中间思维向解决问题的进展（图2,4,6）。通过LM自我评估和深思熟虑实现搜索启发式是新颖的，因为以前的搜索启发式要么是编程的，要么是学习的。最后，我们将这种基于语言的能力与搜索算法（如广度优先搜索（BFS）或深度优先搜索（DFS））结合起来，这些算法允许系统地探索思维树，并进行前瞻和回溯。

从经验上讲，我们提出了三个新问题，即使是使用最先进的语言模型GPT-4[20]，也挑战现有的LM推理方法：24点游戏、创意写作和纵横字谜（表1）。这些任务需要演绎的、数学的、常识性的、词汇推理能力，以及一种结合系统规划或搜索的方法。我们展示了ToT通过足够通用和灵活，以支持不同层次的思维、生成和评估思维的不同方式，以及适应不同问题性质的不同搜索算法，在所有三个任务上都取得了优越的结果。我们还分析了这些选择如何通过系统消融影响模型性能，并讨论了更好地训练和使用LMs的未来方向。

## 2 背景

我们首先对一些现有的使用大型语言模型进行问题解决的方法进行形式化描述，我们的方法受到这些方法的启发，并将在后面与它们进行比较。我们使用 \(p_{\theta}\) 来表示带有参数 \(\theta\) 的预训练语言模型，使用**小写字母**\(x,y,z,s,\cdots\)**表示语言序列**，即 \(x=(x[1],\cdots,x[n])\)，其中每个 \(x[i]\) 是一个令牌，因此 \(p_{\theta}(x)=\prod_{i=1}^{n}p_{\theta}(x[i]|x[1...i])\)。我们使用大写字母 \(S,\cdots\) 来表示语言序列的集合。

**输入-输出（IO）提示**是将问题输入 \(x\) 转换为语言模型的输出 \(y\) 的最常见方法：\(y\sim p_{\theta}(y|\texttt{prompt}_{IO}(x))\)，其中 \(\texttt{prompt}_{IO}(x)\) 将输入 \(x\) 与任务指令和/或少量输入输出示例包装在一起。为简单起见，我们定义 \(p_{\theta}^{\text{prompt}}(\texttt{output}\mid\texttt{input})=p_{\theta}( \texttt{output}\mid\texttt{prompt}(\texttt{input}))\)，这样IO提示可以被表述为 \(y\sim p_{\theta}^{IO}(y|x)\)。

图1：用LLMs解决问题的各种方法的示意图。每个矩形框代表一个_思维_，这是一个连贯的语言序列，作为解决问题的中间步骤。在图2、4、6中查看如何生成、评估和搜索思维的具体示例。

**思维链（CoT）提示**[35] 被提出来解决输入 \(x\) 到输出 \(y\) 的映射非常复杂的情况（例如，当 \(x\) 是一个数学问题而 \(y\) 是最终的数值答案时）。关键思想是引入一系列_思维_ \(z_{1},\cdots,z_{n}\) 来连接 \(x\) 和 \(y\)，其中每个 \(z_{i}\) 是一个连贯的语言序列，作为解决问题的有意义的中间步骤（例如，\(z_{i}\) 可能是数学问答的中间方程）。要使用CoT解决问题，每个思维 \(z_{i}\sim p_{\theta}^{CoT}(z_{i}\mid x,z_{1\dots i-1})\) 是按顺序抽取的，然后输出 \(y\sim p_{\theta}^{CoT}(y|x,z_{1\dots n})\)。实际上，\([z_{1\dots n},y]\sim p_{\theta}^{CoT}(z_{1\dots n},y|x)\) 作为一个连续的语言序列进行抽样，思维的**分解**（例如，每个 \(z_{i}\) 是一个短语、一个句子还是一个段落）是模糊的。

**与CoT的自我一致性（CoT-SC）**[33] 是一种集成方法，它抽取 \(k\) 个独立同分布的思维链：\([z_{1\dots n}^{(i)},y^{(i)}]\sim p_{\theta}^{CoT}(z_{1\dots n},y|x)\)\((i=1\cdots k)\)，然后返回最频繁的输出：\(\operatorname*{arg\,max}_{y}\#\{i\mid y^{(i)}=y\}\)。CoT-SC在CoT的基础上进行了改进，因为对于同一个问题通常有不同的思考过程（例如，证明同一个定理的不同方法），并且通过探索更丰富的思维集合，输出决策可以更加忠实。然而，在每个链中没有对不同思维步骤的局部探索，而且“最频繁”的启发式仅在输出空间有限时适用（例如，多选问答）。

以下是所提供内容的翻译：

## 3 思维树：利用语言模型进行深思熟虑的问题解决

_真正的问题解决过程涉及重复使用可用信息以启动探索，这反过来又揭示了更多信息，直到最终发现解决方案的方法。_ ——_Newell 等人_ [18]

关于人类问题解决的研究表明，人们会在一个组合问题空间中进行搜索 - 一个树状结构，其中节点代表部分解决方案，分支对应于修改它们的运算符[18, 19]。采取哪个分支是由帮助导航问题空间并指导问题解决者朝着解决方案方向的启发式规则确定的。这个视角强调了使用语言模型（LMs）解决一般问题的现有方法的两个关键缺点：1）在局部，它们不探索思考过程中的_不同_延续——树的分支。2）在全局，它们没有整合任何类型的规划、前瞻或回溯来帮助评估这些不同的选项——这种启发式引导搜索似乎是人类问题解决的特点。

为了解决这些缺点，我们引入了_思维树（ToT）_，一种允许语言模型通过思维探索多条推理路径的范式（图1（c））。ToT将任何问题框架为对树的搜索，其中每个节点是一个**状态** \(s=[x,z_{1...i}]\)，代表到目前为止的输入和思考序列的部分解决方案。ToT的一个具体实例涉及回答四个问题：1. 如何将中间过程**分解**成思考步骤；2. 如何从每个状态**生成**潜在的思考；3. 如何启发式地**评估**状态；4. 使用什么**搜索**算法。

**1. 思考分解。**虽然CoT连贯地抽样思考，而不进行明确的分解，但ToT利用问题属性来设计和分解中间思考步骤。如表1所示，根据不同的问题，一个思考可能是几个词（纵横字谜），一个方程式（24点游戏），或者一个整段的写作计划（创意写作）。通常，一个思考应该足够“小”，以便LMs能够生成有前途和多样的样本（例如，生成一整本书通常太“大”而无法保持连贯性），但又足够“大”，以便LMs可以评估其解决问题的前景（例如，生成一个标记通常太“小”而无法评估）。

**2. 思考生成器 \(G(p_{\theta},s,k)\)。**给定一个树状态 \(s=[x,z_{1...i}]\)，我们考虑生成下一个思考步骤的\(k\)个候选者的两种策略：

1. 从CoT提示中**抽样**独立同分布(i.i.d.)的思考（创意写作，图4）：\(z^{(j)}\sim p_{\theta}^{CoT}(z_{i+1}|s)=p_{\theta}^{CoT}(z_{i+1}|x,z_{1...i})\)\((j=1...k)\)。当思考空间丰富时（例如，每个思考是一个段落），i.i.d.样本会带来多样性；
2. 使用“提议提示”顺序**提出**思考（24点游戏，图2；纵横字谜，图6）：\([z^{(1)},...,z^{(k)}]\sim p_{\theta}^{propose}(z_{i+1}^{(1...k)}|s)\)。当思考空间更受限时（例如，每个思考只是一个单词或一行），在同一上下文中提出不同的思考可以避免重复。

**3. 状态评估器 \(V(p_{\theta},S)\)。**给定不同状态的前沿，状态评估器评估它在解决问题方面的进展，作为搜索算法的_启发式_，以确定保持探索的状态和顺序。虽然启发式是解决搜索问题的标准方法，但它们通常要么是编程的（例如DeepBlue [3]），要么是学习的（例如AlphaGo [26]）。我们提出了第三种替代方案，通过使用LM有意识地思考状态。在适用的情况下，这样的深思熟虑的启发式可以比编程规则更灵活，比学习模型更节省样本。类似于思考生成器，我们考虑两种策略来独立或共同评估状态：

1. 独立**评估**每个状态：\(V(p_{\theta},S)(s)\sim p_{\theta}^{value}(v|s)\)\(\forall s\in S\)，其中一个值提示基于状态 \(s\) 生成一个标量值 \(v\)（例如，1-10）或分类（例如，确定/可能/不可能），可以通过启发式转换为一个值。这种评估性推理的基础可以在不同的问题和思考步骤中变化。在这项工作中，我们通过少量的_前瞻_模拟（例如，快速确认5、5、14可以通过5 + 5

 * 14 = 75来加以解决）和_后瞻_回顾（例如，5 + 5 * 14 = 75 = 3 * 25，所以25可能是一个好的下一步目标）来设计启发式。

2. **对比**状态以**相对评估**它们：\(V(p_{\theta},S)(S)\sim p_{\theta}^{compare}(v|S)\)，其中一个比较提示基于一组状态 \(S\) 生成一个排名或首选项，可以通过启发式转换为值。这种方法利用了LM的强烈理解能力，可以在不同的状态之间进行比较，而不是评估它们的绝对价值。

**4. 搜索算法。**最后，我们需要一个搜索算法来导航思考树。虽然ToT是算法不可知的，但对于每个问题，我们选择最适合其特定要求和约束的算法。例如，我们使用最佳优先搜索（BeFS）来解决24点游戏（图2），而对于创意写作（图4）和纵横字谜（图6），我们选择了一种启发式的深度优先搜索（HeuDFS）。尽管这些选择都是基于我们对每个问题的理解，但ToT的设计使其可以容易地适应新问题和搜索策略。

通过结合这四个元素，ToT构建了一个问题解决的思维模型，能够在多个思维路径之间导航和选择，类似于人类解决复杂问题的方式。我们将在接下来的部分中展示ToT在多个领域的应用，从游戏和谜题到科学和创意写作。

## 4 实验

我们提出了三项任务，即使是使用最先进的语言模型GPT-4 [20]进行抽样，通过标准的输入输出提示或思维链（CoT）提示，这些任务也是困难的。我们展示了如何在思维树（ToT）中进行深思熟虑的搜索以产生更好的结果，并且更重要的是，我们发现了使用语言模型解决需要搜索或规划的问题的新颖且有前景的方法。除非另有说明，否则我们使用Chat Completion模式的GPT-41进行实验，抽样温度为0.7。

脚注1：实验是在2023年5月5日至16日之间进行的。

### 24点游戏

24点游戏是一项数学推理挑战，目标是使用4个数字和基本算术运算（+-*/）得出24。例如，给定输入“4 9 10 13”，一个可能的解决方案输出是“（10 - 4）*（13 - 9）= 24”。

**任务设置。** 我们从4nums.com网站上抓取数据，该网站有1,362个游戏，这些游戏按人类解决时间的难易程度进行了排序，并使用索引在901-1,000之间的相对困难的游戏子集进行测试。对于每项任务，如果输出是一个有效的等式，等于24，并且每个输入数字只使用一次，我们就认为该输出是成功的。我们将在100个游戏中的成功率报告为指标。

**基线。** 我们使用一个标准的输入输出（IO）提示，其中包含5个上下文示例。对于思维链（CoT）提示，我们增加了每个输入输出对中的3个中间方程，每个方程都对剩下的两个数字进行操作。例如，给定输入“4 9 10 13”，思维过程可能是“13 - 9 = 4（剩下：4 4 10）; 10 - 4 = 6（剩下：4 6）; 4 * 6 = 24（剩下：24）”。对于每个游戏，我们对IO和CoT提示进行100次抽样以得出平均性能。我们还考虑了一个CoT自洽性基线，它从100个CoT样本中获取大部分输出，并在IO样本的基础上进行了最多\(10\)次的迭代细化方法。在每次迭代中，如果输出不正确，LM将根据所有以前的历史记录进行调整，以“反思您的错误并生成一个改进的答案”。请注意，它使用关于方程正确性的真实反馈信号。

**ToT设置。** 要将24点游戏纳入ToT，将思维分解为3个步骤是很自然的，每个步骤都是一个中间方程。如图2（a）所示，每个树节点，我们都会提取“剩下”的数字，并提示LM提出一些可能的下一步。同一个“提案提示”用于所有3个思考步骤，尽管它只有一个包含4个输入数字的示例。我们在ToT中进行宽度优先搜索（BFS），每一步我们保留最佳的\(b=5\)候选者。为了在ToT中进行深思熟虑的BFS，如图2（b）所示，我们提示LM评估每个思考候选者为“确定/可能/不可能”，以达到24的目标。目的是促进可以在几次预查试验中判决的正确部分解决方案，并根据“太大/太小”的常识消除不可能的部分解决方案，并保留其余的“可能”。我们对每个想法进行\(3\)次取样。

\begin{table}
\begin{tabular}{l|l l l} \hline \hline  & **24点游戏** & **创意写作** & **5x5纵横字谜** \\ \hline
**输入** & 4个数字 (4 9 10 13) & 4个随机句子 & 10个提示 (h1.呈现;..) \\ \hline
**输出** & 一个等式达到24 (13-9)*(10-4)=24 & 一个包含4段结尾是4个句子的段落 & 5x5字母：SHOWN; WIRRA; AVAIL;... \\ \hline
**思维** & 3个中间方程 (13-9=4 (剩下 4,4,10); 10-4=6 (剩下 4,6

); 4*6=24) & 一个简短的写作计划 (1.介绍一个连接的书...) & 用于提示的填词：(h1.显示; v5. naled;...) \\ \hline
**\#ToT步骤** & 3 & 1 & 5-10（可变） \\ \hline \hline \end{tabular}
\end{table}
表1：任务概览。输入，输出，思维实例以蓝色显示。

图2：24点游戏中的ToT。LM被提示进行（a）思维生成和（b）评估。

**结果。** 如表2所示，IO，CoT和CoT-SC提示方法在任务上的表现不佳，仅达到7.3％，4.0％和9.0％的成功率。相比之下，ToT的宽度为\(b=1\)已经达到\(45\%\)的成功率，而\(b=5\)达到\(74\%\)。我们还考虑了IO/CoT的一个先知设置，通过使用最好的\(k\)样本（\(1\leq k\leq 100\)）来计算成功率。为了比较IO/CoT（最好的k）和ToT，我们考虑计算ToT中每项任务访问的树节点数\(b=1\cdots 5\)，并在图3（a）中映射5个成功率，将IO/CoT（最好的\(k\)）视为在一个老虎机中访问\(k\)个节点。毫不奇怪，CoT的扩展性优于IO，最好的100个CoT样本达到了\(49\%\)的成功率，但仍然远不如在ToT中探索更多节点（\(b>1\)）。

**错误分析。** 图3（b）分解了CoT和ToT样本在哪一步失败了任务，即思维（在CoT中）或所有\(b\)思维（在ToT中）是无效的或无法达到24。值得注意的是，大约60％的CoT样本在生成第一步后就已经失败了任务，或者等效地，是前三个词（例如，“\(4+9\)”）。这突显了直接从左到右解码的问题。

### 创意写作

接下来，我们设计了一个创意写作任务，输入是4个随机句子，输出应该是一个包含4段的连贯文章，每段分别以这4个输入句子结束。这样的任务是开放式的，具有探索性，并挑战创造性思维和高级规划能力。

**任务设置。** 我们从randomwordgenerator.com随机抽取句子，形成100个输入，每个输入约束都没有标准答案。由于我们发现GPT-4大多数时候可以遵循输入约束，我们专注于通过两种方式评估段落的连贯性：使用GPT-4的零镜头提示提供1-10的标量评分，或使用人类判断比较不同方法的输出对。对于前者，我们对每个任务输出进行5次评分并取平均，这5个评分通常是一致的，平均输出的标准偏差约为\(0.56\)。对于后者，我们在盲研中雇用了部分作者来比较CoT和ToT生成的段落对的连贯性，段落的顺序在100个输入中随机翻转。

**基准线。** 鉴于任务的创造性质，IO和CoT提示都是零镜头的。前者提示LM直接根据输入约束生成连贯的段落，后者则提示LM首先制定一个简短的计划，然后编写段落，即计划用作中间思考步骤。我们为每项任务生成10个IO和CoT样本。我们还考虑了一种在每项任务的随机IO样本上进行迭代细化（\(k\leq 5\)）的方法，其中LM基于输入约束和上一次生成的段落来决定是否段落已经“完全连贯”，如果不是，则生成一个更精细的版本。

**ToT设置。** 我们构建了一个深度为2的ToT（并且只有1个中间思考步骤）——LM首先生成\(k=5\)个计划并为最佳计划投票（图4），然后类似地根据最佳计划生成\(k=5\)个段落，然后为最佳段落投票。这里的宽度限制是\(b=1\)，因为每步只保留一个选择。一个简单的零镜头投票提示（“分析下面的选择，然后得出哪个是对指令最有希望的结论”）被用来在两个步骤中各抽样5票。

**结果。** 图5（a）显示了100个任务中GPT-4的平均分数，其中ToT（7.56）被认为比IO（6.19）和CoT（6.93）生成了更连贯的段落。尽管这样的自动度量可能有噪音，但图5（b）通过显示人类在100个段落对中有41个更喜欢ToT超过CoT，而只有21个更喜欢CoT超过ToT（其他38对被发现“相似连贯”）来证实这一发现。最后，迭代细化在这种自然语言任务上更有效，其中

\begin{table}
\begin{tabular}{l l} \hline \hline
**方法** & **成功率** \\ \hline IO提示 & 7.3\% \\ CoT提示 & 4.0\% \\ CoT-SC (k=100) & 9.0\% \\ ToT（我们的）(b=1) & 45\% \\ ToT（我们的）(b=5) & **74\%** \\ \hline IO + Refine (k=10) & 27\% \\ IO（最好的100个中的一个） & 33\% \\ CoT（最好的100个中的一个） & 49\% \\ \hline \hline \end{tabular}
\end{table}
表2：24点游戏结果。图3：24点游戏（a）规模分析和（b）错误分析。

它将IO连贯性分数从6.19提高到7.67，并将ToT连贯性分数从7.56提高到7.91。

